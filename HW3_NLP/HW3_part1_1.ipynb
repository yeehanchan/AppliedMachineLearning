{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import rlcompleter, readline\n",
    "readline.parse_and_bind('tab: complete')\n",
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "amazon_data = []\n",
    "with open(\"sentiment_labelled_sentences/amazon_cells_labelled.txt\") as f:\n",
    "    for line in f:\n",
    "        amazon_data.append(line.strip('\\n').split('\\t'))\n",
    "amazon_data = np.asarray(amazon_data)\n",
    "\n",
    "imdb_data = []\n",
    "with open(\"sentiment_labelled_sentences/imdb_labelled.txt\") as f:\n",
    "    for line in f:\n",
    "        imdb_data.append(line.strip('\\n').split('\\t'))\n",
    "imdb_data = np.asarray(imdb_data)\n",
    "\n",
    "yelp_data = []\n",
    "with open(\"sentiment_labelled_sentences/yelp_labelled.txt\") as f:\n",
    "    for line in f:\n",
    "        yelp_data.append(line.strip('\\n').split('\\t'))\n",
    "yelp_data = np.asarray(yelp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_data(data):\n",
    "    sentence = []\n",
    "    score = []\n",
    "    line = \"\"\n",
    "#    print type(line)\n",
    "    for row in data:\n",
    "#        print type(row[0])\n",
    "#        line = line + row[0]\n",
    "#        print line\n",
    "        if row[1] !='':\n",
    "            sentence.append(row[0])\n",
    "            score.append(int(row[1]))\n",
    "#            line = \"\"\n",
    "    return [sentence,score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parse data\n",
    "[amazon_sentence,amazon_score] = parse_data(amazon_data)\n",
    "[imdb_sentence,imdb_score] = parse_data(imdb_data)\n",
    "[yelp_sentence,yelp_score] = parse_data(yelp_data)\n",
    "#len([i for i in amazon_score if i==1])\n",
    "#len([i for i in imdb_score if i==1])\n",
    "len([i for i in yelp_score if i==1])\n",
    "# the labels 0 and 1 is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lowercase \n",
    "#lowercase text data in case of program misunderstand same words as different words.\n",
    "amazon_sentence = np.char.lower(amazon_sentence)\n",
    "imdb_sentence = np.char.lower(imdb_sentence)\n",
    "yelp_sentence = np.char.lower(yelp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "#segment sentence into words.\n",
    "from nltk import word_tokenize\n",
    "def tokenizer(data):\n",
    "    token_data = []\n",
    "    for line in data:\n",
    "        line = word_tokenize(line)\n",
    "        token_data.append(line)\n",
    "    return np.asarray(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_token = tokenizer(amazon_sentence)\n",
    "i_token = tokenizer(imdb_sentence)\n",
    "y_token = tokenizer(yelp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#strip punctuation\n",
    "#strip punctuation because punctuation dosen't make contribution to clustering\n",
    "import string\n",
    "def strip_punctuation(data):\n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation.extend(['``','\"\"','...'])\n",
    "#    print punctuation\n",
    "#    for each in punctuation:\n",
    "#        index = np.where(data == 'so')\n",
    "#    return index\n",
    "#    print data.shape\n",
    "    strip = []\n",
    "    for line in data:\n",
    "        tmp = np.in1d(line,punctuation)\n",
    "        index = np.where(tmp == True)\n",
    "        strip_line = np.delete(line,index)\n",
    "        strip.append(strip_line)\n",
    "    return np.asarray(strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_strip = strip_punctuation(a_token)\n",
    "i_strip = strip_punctuation(i_token)\n",
    "y_strip = strip_punctuation(y_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stopwords\n",
    "#same reason with punctuation\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw.extend(['\\xc2\\x96'])\n",
    "print sw\n",
    "#print sw\n",
    "def remove_stopwords(data):\n",
    "    rm_data = []\n",
    "    for line in data:\n",
    "        tmp = np.in1d(line,sw)\n",
    "#        print tmp\n",
    "        index = np.where(tmp == True)\n",
    "        rm_line = np.delete(line,index)\n",
    "        rm_data.append(rm_line)\n",
    "#        print rm_line\n",
    "    return rm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_rm = remove_stopwords(a_strip)\n",
    "#np.delete(i_strip,np.where(i_strip == '\\xc2\\x96'.decode('utf-8')))\n",
    "i_rm = remove_stopwords(i_strip)\n",
    "y_rm = remove_stopwords(y_strip)\n",
    "#print i_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "#words with different type but mean same thing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatizer(data):\n",
    "    l = WordNetLemmatizer()\n",
    "    lem_data = []\n",
    "    lem_line = []\n",
    "    for line in data:\n",
    "        for item in line:\n",
    "            lem_line.append(l.lemmatize(item))\n",
    "        print lem_line\n",
    "        lem_data.append(lem_line)\n",
    "        lem_line = []\n",
    "    return np.asarray(lem_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_lem = lemmatizer(a_rm)\n",
    "i_lem = lemmatizer(i_rm)\n",
    "y_lem = lemmatizer(y_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(sys)\n",
    "#split training and testing set\n",
    "def trainingAndTesting(sentence,score):\n",
    "    score = np.asarray(score)\n",
    "    positive = sentence[np.where(score == 1)]\n",
    "    negative = sentence[np.where(score == 0)]\n",
    "    train = np.concatenate((positive[:1200],negative[:1200]),axis=0)\n",
    "    train_label = [1]*1200+[0]*1200\n",
    "    test = np.concatenate((positive[1200:],negative[1200:]),axis=0)\n",
    "    return train,test,train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = np.concatenate((a_lem,i_lem,y_lem),axis=0)\n",
    "score = amazon_score+imdb_score+yelp_score\n",
    "train,test,train_label = trainingAndTesting(sentence,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(dic == 'packed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Bag of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4361"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build up dict through training set\n",
    "def build_dic(train):\n",
    "    dic = []\n",
    "    for line in train:\n",
    "        for word in line:\n",
    "            dic.append(word)\n",
    "    dic = list(set(dic))\n",
    "    return dic\n",
    "dic = build_dic(train)\n",
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build up feature vector\n",
    "#testing set might include data that doen't occur in training data. That would influence the performance of training.\n",
    "def build_vector(train):\n",
    "    vectors = []\n",
    "    for line in train:\n",
    "        vector = [0]*len(dic)\n",
    "        for word in line:\n",
    "            if word in dic:\n",
    "                vector[dic.index(word)] += 1\n",
    "        vectors.append(vector)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vectors = build_vector(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_vectors = build_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#report any two feature vectors of reviews in training set\n",
    "#train_vectors[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('label.txt',train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('train_feature_vectors.txt',train_vectors)\n",
    "np.savetxt('test_feature_vectors.txt',test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[405, 417, 455]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(train_vectors)) if np.sum(train_vectors[i]) == 0]\n",
    "[i for i in range(len(test_vectors)) if np.sum(test_vectors[i]) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('train_dic.txt','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for line in train:\n",
    "        writer.writerow(line)\n",
    "        \n",
    "with open('test_dic.txt','w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for line in test:\n",
    "        writer.writerow(line)\n",
    "#with open('dic.txt','w') as f:\n",
    "#    writer = csv.writer(f)\n",
    "#    writer.writerows(dic)\n",
    "#np.savetxt('train_dic.txt',train)\n",
    "#np.savetxt('test_dic.txt',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4361,)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = np.asarray(dic,dtype = 'str')\n",
    "dic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('dic.txt',dic,fmt=\"%s\")\n",
    "np.savetxt('train_dic.txt',train,fmt=\"%s\")\n",
    "np.savetxt('test_dic.txt',test,fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
